DATA:
  data_name: kitti
  data_root: Dataset/
  # data_root: data/Kitti

TRAIN:
  # Network Architecture
  # arch: TransformerCalib
  # num point: 7808 OK for BS8,16GPU FSDP

  img_depth: 3              # Fixed parameter. Avoid modify
  pc_feat_dim: 1
  num_points:   8192  #7808 #62464 # 31232 8192  
  resized_img: 

  mlp_blocks: 2             # For Attention in both Vit and PCtrs
  mlp_feature: 64
  mlp_heads: 2
  mlp_hidden: 64 
  mlp_dropout: 0.1 

  rt_channels: 16 #384      # Rotation Translation gen network
  rt_hidden_size: 128      
  rt_Dropout: 0.7  

  cam_patch_dim: 8          # Vit Configurations
  cam_patch_method: 1                       # 1,2,3
  cam_conv_channels: 8      
  cam_position_en_method: 1                 # 1,2

  pc_conv_channels: 3       # PCtrs Configurations
  pc_conv_blocks: 1                         ### problem when > 1
  pc_pool_type: null                        # None, max, avg
  pc_out_size: null                         # None or (M,N)
  pc_cluster_window_size: 0.5               #2?
  pc_nei_quer_radius: 200         # tp.ball_query
  num_neighbor: 200
  prev_grid_size: 0.04            # KPConv
  sigma: 1.0

  # training
  batch_size: 8     # 60 batch size for training
  epochs: 20
  optimizer: AdamW 
  learning_rate: 5e-4  #0.006
  momentum: 0.9
  weight_decay: 0.01  #todo: 0.0001
  sche_step_size: 20
  sche_gamma: 0.1
  num_recursive_iter: 1

  multi_gpu_tr:  true
  data_parallel: false  # w/o multi_gpu_tr,need one node only.
  gpu_tracking: true

LOG_CKP:
  save_writter_path: result/torchlogs/
  logging_type: tensorboard      #all, tensorboard, wandb, comet_ml, None
  print_log_freq: 5
  save_ckp_path: result/checkpoints2/
  save_ckp_freq: 5
  prj_dir: /project/p_calibrrn/result
  resume_from_checkpoint: false


TEST:
  batch_size_test: 4
